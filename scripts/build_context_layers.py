#!/usr/bin/env python3
"""
Build static context layers for a given region.

Reads from:
  regions/profiles/insight.<region>.yml ‚Üí to get bbox + crop list

Fetches:
  - üåæ Crop phenology (via Open-Meteo temperature/GDD)
  - ü™± Soil properties (SoilGrids v2.0)
  - üèîÔ∏è Elevation (Open-Elevation API)
  - üîÅ Dynamic layers via scripts/fetch_all.py (CHIRPS, NDVI, SMAP, OpenMeteo)

Outputs:
  data/<region>/context_layers/
    soil.csv, soil_metadata.json
    topography.csv, topography_metadata.json
    phenology_<crop>.csv, phenology_<crop>_metadata.json
"""

import argparse
import json
from datetime import datetime, timezone
from pathlib import Path
from subprocess import run
import sys

try:  # Local import to avoid circular dependency when used as a module
    from scripts.run_pipeline import require
except ModuleNotFoundError:  # pragma: no cover - fallback for direct execution
    from run_pipeline import require  # type: ignore

np = require("numpy")
if np is None:
    raise RuntimeError(
        "NumPy is required for build_context_layers. "
        "Re-run without OFFLINE_MODE to install missing dependencies."
    )

pd = require("pandas")
if pd is None:
    raise RuntimeError(
        "Pandas is required for build_context_layers. "
        "Re-run without OFFLINE_MODE to install missing dependencies."
    )

requests = require("requests")
if requests is None:
    raise RuntimeError(
        "Requests is required for build_context_layers. "
        "Re-run without OFFLINE_MODE to install missing dependencies."
    )

yaml = require("pyyaml", "yaml")
if yaml is None:
    raise RuntimeError(
        "PyYAML is required for build_context_layers. "
        "Re-run without OFFLINE_MODE to install missing dependencies."
    )

from _shared import load_region_profile, resolve_region_config_path


# -------------------------------------------------------
# üåæ Phenology Helper (Open-Meteo ‚Üí GDD)
# -------------------------------------------------------
def fetch_openmeteo_phenology(lat: float, lon: float, crop: str, out_dir: Path) -> pd.DataFrame:
    """
    Derive simple phenology proxy from Open-Meteo ERA5 reanalysis.
    Uses temperature-based GDD accumulation to estimate planting,
    flowering, and harvest DOY thresholds.
    """

    url = "https://archive-api.open-meteo.com/v1/era5"
    params = {
        "latitude": lat,
        "longitude": lon,
        "start_date": "2010-01-01",
        "end_date": datetime.now().strftime("%Y-%m-%d"),
        "daily": ["temperature_2m_max", "temperature_2m_min"],
        "timezone": "auto",
    }

    meta = {
        "timestamp": datetime.now(timezone.utc).isoformat(),
        "lat": lat,
        "lon": lon,
        "crop": crop,
        "source": None,
    }

    # Crop-specific base temps (¬∞C) for GDD calculation
    base_temp_map = {
        "maize": 10, "corn": 10,
        "soybean": 8, "wheat": 0,
        "rice": 10, "coffee": 12,
        "cotton": 15, "sorghum": 8,
        "vegetables": 8, "tomato": 10,
        "cowpea": 10, "generic_crop": 10
    }

    # ‚úÖ FIX: Safely handle dict crop inputs (prevents AttributeError)
    if isinstance(crop, dict):
        crop_name = crop.get("name", "unknown")
    else:
        crop_name = str(crop)
    base_temp = base_temp_map.get(crop_name.lower(), 10)

    try:
        r = requests.get(url, params=params, timeout=60)
        r.raise_for_status()
        data = r.json()
        temps = pd.DataFrame({
            "date": data["daily"]["time"],
            "tmax": data["daily"]["temperature_2m_max"],
            "tmin": data["daily"]["temperature_2m_min"],
        })
        temps["tavg"] = (temps["tmax"] + temps["tmin"]) / 2
        temps["gdd"] = (temps["tavg"] - base_temp).clip(lower=0)
        temps["cumsum_gdd"] = temps["gdd"].cumsum()

        # Approx phenology thresholds (% of cumulative GDD)
        total_gdd = temps["cumsum_gdd"].iloc[-1]
        thresholds = {
            "planting": 0.05 * total_gdd,
            "flowering": 0.55 * total_gdd,
            "harvest": 0.95 * total_gdd,
        }

        phenology = {}
        for key, gdd_threshold in thresholds.items():
            subset = temps[temps["cumsum_gdd"] >= gdd_threshold]
            phenology[f"{key}_date"] = (
                subset["date"].iloc[0] if not subset.empty else np.nan
            )

        df = pd.DataFrame([{
            "crop": crop,
            "planting_date": phenology.get("planting_date"),
            "flowering_date": phenology.get("flowering_date"),
            "harvest_date": phenology.get("harvest_date"),
            "lat": lat,
            "lon": lon,
            "data_source": "OpenMeteo_GDD",
        }])
        meta["source"] = "OpenMeteo_GDD"

    except Exception as e:
        print(f"‚ö†Ô∏è Phenology fallback for {crop}: {e}")
        df = pd.DataFrame([{
            "crop": crop,
            "planting_date": np.nan,
            "flowering_date": np.nan,
            "harvest_date": np.nan,
            "lat": lat,
            "lon": lon,
            "data_source": "Fallback_None",
        }])
        meta["source"] = "Fallback_None"

    out_dir.mkdir(parents=True, exist_ok=True)
    out_file = out_dir / f"phenology_{crop}.csv"
    df.to_csv(out_file, index=False)
    print(f"‚úÖ Saved {out_file.name} ({len(df)} rows, source={meta['source']})")

    meta_path = out_dir / f"phenology_{crop}_metadata.json"
    with open(meta_path, "w") as f:
        json.dump(meta, f, indent=2)

    return df


# -------------------------------------------------------
# ü™± SoilGrids v2.0 & Elevation Helpers
# -------------------------------------------------------
def fetch_soilgrids(lat: float, lon: float, out_path: Path) -> pd.DataFrame:
    """
    Fetch soil property means from the SoilGrids v2.0 API.
    Automatically retries smaller property sets if the server returns 500,
    and falls back to FAO defaults if no valid values are found.
    """
    import time

    url = "https://rest.isric.org/soilgrids/v2.0/properties/query"
    headers = {"User-Agent": "RegionalAgent/1.0"}
    properties = ["clay", "silt", "sand", "soc", "phh2o", "bdod"]
    depths = "0-5cm,5-15cm,15-30cm"

    meta = {
        "timestamp": datetime.now(timezone.utc).isoformat(),
        "lat": lat,
        "lon": lon,
        "source": None,
    }

    def query_soilgrids(prop_list):
        params = {
            "lon": lon,
            "lat": lat,
            "property": ",".join(prop_list),
            "depth": depths,
        }
        r = requests.get(url, params=params, headers=headers, timeout=30)
        r.raise_for_status()
        return r.json()

    try:
        try:
            # First attempt with all properties together
            r = requests.get(
                url,
                params={
                    "lon": lon,
                    "lat": lat,
                    "property": ",".join(properties),
                    "depth": depths,
                },
                headers=headers,
                timeout=30,
            )
            r.raise_for_status()
            data = r.json()
        except Exception as e:
            print(f"‚ö†Ô∏è SoilGrids bulk query failed ({e}); retrying smaller sets...")
            data = {"properties": {"layers": []}}
            for p in properties:
                try:
                    time.sleep(1)
                    partial = query_soilgrids([p])
                    layers = partial.get("properties", {}).get("layers", [])
                    if isinstance(layers, dict):
                        layers = [layers]
                    data["properties"]["layers"].extend(layers)
                    print(f"‚úÖ Retrieved {p}")
                except Exception as sub_e:
                    print(f"‚ö†Ô∏è Skipped {p}: {sub_e}")
                    continue

        props = data.get("properties", {}).get("layers", {})
        if isinstance(props, dict):
            layer_items = props.items()
        elif isinstance(props, list):
            layer_items = [(layer.get("name"), layer) for layer in props if isinstance(layer, dict)]
        else:
            raise ValueError("Unrecognized SoilGrids format")

        records = []
        for name, layer in layer_items:
            depths = layer.get("depths", [])
            values = [
                d.get("values", {}).get("mean")
                for d in depths
                if d.get("values", {}).get("mean") is not None
            ]
            if values:
                records.append({"property": name, "mean_value": sum(values) / len(values)})

        if not records:
            raise ValueError("No valid mean values extracted from SoilGrids response.")

        df = pd.DataFrame(records)
        df = df.pivot_table(values="mean_value", columns="property", aggfunc="first").reset_index(drop=True)
        df.rename(columns={
            "clay": "clay_pct", "silt": "silt_pct", "sand": "sand_pct",
            "soc": "soc_gkg", "phh2o": "ph", "bdod": "bulk_density",
        }, inplace=True)

        df["data_source"] = "SoilGrids_API"
        meta["source"] = "SoilGrids_API"

    except Exception as e:
        print(f"‚ö†Ô∏è SoilGrids error: {e}")
        df = pd.DataFrame([{
            "clay_pct": 25, "silt_pct": 35, "sand_pct": 40, "soc_gkg": 15,
            "ph": 6.5, "bulk_density": 1.3, "data_source": "Fallback_FAO_Defaults",
        }])
        meta["source"] = "Fallback_FAO_Defaults"

    out_path.parent.mkdir(parents=True, exist_ok=True)
    df.to_csv(out_path, index=False)
    with open(out_path.with_name("soil_metadata.json"), "w") as f:
        json.dump(meta, f, indent=2)

    print(f"‚úÖ Saved {out_path.name} ({len(df)} rows, {df.shape[1]} cols, source={meta['source']})")
    return df



# -------------------------------------------------------
# üß© Context Layer Builder
# -------------------------------------------------------
def build_context_layers(region_name: str):
    root = Path(__file__).resolve().parents[1]
    cfg_path = resolve_region_config_path(region_name)
    region_dir = root / "data" / region_name
    ctx_dir = region_dir / "context_layers"
    ctx_dir.mkdir(parents=True, exist_ok=True)

    cfg = load_region_profile(region_name)
    bbox = cfg.get("region_meta", {}).get("bbox", [None, None, None, None])
    crops = cfg.get("region_meta", {}).get("crops", ["generic_crop"])

    if not bbox or len(bbox) != 4 or any(v is None for v in bbox):
        print("‚ö†Ô∏è Invalid or missing bbox; using (0,0).")
        lat, lon = 0.0, 0.0
    else:
        lat = float(np.mean([bbox[1], bbox[3]]))
        lon = float(np.mean([bbox[0], bbox[2]]))

    print(f"üåç Building context layers for {region_name}")
    print(f"üìç Approx centroid: lat={lat:.3f}, lon={lon:.3f}")
    print("ü™± Fetching SoilGrids data...")
    fetch_soilgrids(lat, lon, ctx_dir / "soil.csv")

    print("üèîÔ∏è Fetching elevation data...")
    fetch_elevation(lat, lon, ctx_dir)

    for crop in crops:
        print(f"üåæ Fetching phenology for {crop}...")
        fetch_openmeteo_phenology(lat, lon, crop, ctx_dir)

    # üîÅ Run dynamic fetchers
    fetch_all = root / "scripts" / "fetch_all.py"
    if fetch_all.exists():
        print("üîÅ Running fetch_all.py for dynamic datasets...")
        run([sys.executable, str(fetch_all), "--region", region_name, "--mode", "active"], check=False)

    cache_builder = root / "scripts" / "build_region_cache.py"
    if cache_builder.exists():
        print("üì¶ Building full regional cache...")
        run([sys.executable, str(cache_builder), "--region", region_name], check=False)

    print(f"üéâ Context layers built successfully for {region_name}")


# -------------------------------------------------------
# Entrypoint
# -------------------------------------------------------
if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Build static context layers for a region.")
    parser.add_argument("--region", required=True, help="Region name (e.g. hungary_farmland)")
    parser.add_argument("--crop", nargs="+", help="Optional crop list to override YAML (e.g. --crop maize wheat)")
    args = parser.parse_args()

    if args.crop:
        cfg_path = resolve_region_config_path(args.region)
        cfg = yaml.safe_load(open(cfg_path))
        cfg.setdefault("region_meta", {})["crops"] = args.crop
        yaml.safe_dump(cfg, open(cfg_path, "w"), sort_keys=False)
        print(f"üåæ Overrode crops in profile ‚Üí {args.crop}")

    build_context_layers(args.region)
